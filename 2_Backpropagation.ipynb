{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "2.Backpropagation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyyyaspatil/androiddevelopment/blob/master/2_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emkr3GSp7AHQ"
      },
      "source": [
        "## 2. Build an Artificial Neural Network by implementing the Back propagation Algorithm and test the same using appropriate data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKVq78KS7AHV"
      },
      "source": [
        "The Backpropagation algorithm is a supervised learning method for multilayer feed-forward networks from the field of Artificial Neural Networks.\n",
        "\n",
        "Feed-forward neural networks are inspired by the information processing of one or more neural cells, called a neuron. A neuron accepts input signals via its dendrites, which pass the electrical signal down to the cell body. The axon carries the signal out to synapses, which are the connections of a cell’s axon to other cell’s dendrites.\n",
        "\n",
        "The principle of the backpropagation approach is to model a given function by modifying internal weightings of input signals to produce an expected output signal. The system is trained using a supervised learning method, where the error between the system’s output and a known expected output is presented to the system and used to modify its internal state.\n",
        "\n",
        "Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input layer, one hidden layer, and one output layer.\n",
        "\n",
        "Backpropagation can be used for both classification and regression problems, but we will focus on classification in this tutorial.\n",
        "\n",
        "In classification problems, best results are achieved when the network has one neuron in the output layer for each class value. For example, a 2-class or binary classification problem with the class values of A and B. These expected outputs would have to be transformed into binary vectors with one column for each class value. Such as [1, 0] and [0, 1] for A and B respectively. This is called a one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBpR34yE7AHW"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3puAVYg7AHX"
      },
      "source": [
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X/np.amax(X,axis=0) # maximum of X array longitudinally\n",
        "y = y/100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCVoAIDe7AHX"
      },
      "source": [
        "#Sigmoid Function\n",
        "def sigmoid (x):\n",
        "    return 1/(1 + np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F7zLizC7AHX"
      },
      "source": [
        "#Derivative of Sigmoid Function\n",
        "def derivatives_sigmoid(x):\n",
        "    return x * (1 - x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj8-uDuM7AHY"
      },
      "source": [
        "#Variable initialization\n",
        "epoch=7000 #Setting training iterations\n",
        "lr=0.1 #Setting learning rate\n",
        "inputlayer_neurons = 2 #number of features in data set\n",
        "hiddenlayer_neurons = 3 #number of hidden layers neurons\n",
        "output_neurons = 1 #number of neurons at output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61jtxUSj7AHY"
      },
      "source": [
        "#weight and bias initialization\n",
        "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
        "wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "bout=np.random.uniform(size=(1,output_neurons))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5UmI5hS7AHY"
      },
      "source": [
        "#draws a random range of numbers uniformly of dim x*y\n",
        "for i in range(epoch):\n",
        "#Forward Propogation\n",
        "    hinp1=np.dot(X,wh)\n",
        "    hinp=hinp1 + bh\n",
        "    hlayer_act = sigmoid(hinp)\n",
        "    outinp1=np.dot(hlayer_act,wout)\n",
        "    outinp= outinp1+ bout\n",
        "    output = sigmoid(outinp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY_ShHfu7AHY",
        "outputId": "8ee083ab-41ad-4bce-be35-d37db77503ab"
      },
      "source": [
        "#Backpropagation\n",
        "EO = y-output\n",
        "outgrad = derivatives_sigmoid(output)\n",
        "d_output = EO* outgrad\n",
        "EH = d_output.dot(wout.T)\n",
        "hiddengrad = derivatives_sigmoid(hlayer_act)#how much hidden layer wts contributed to error\n",
        "d_hiddenlayer = EH * hiddengrad\n",
        "wout += hlayer_act.T.dot(d_output) *lr# dotproduct of nextlayererror and currentlayerop\n",
        "\n",
        "# bout += np.sum(d_output, axis=0,keepdims=True) *lr\n",
        "wh += X.T.dot(d_hiddenlayer) *lr\n",
        "#bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr\n",
        "\n",
        "print(\"Input: \\n\" + str(X))\n",
        "print(\"Actual Output: \\n\" + str(y))\n",
        "\n",
        "print(\"Learned Output: \\n\" ,output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            "[[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "Learned Output: \n",
            " [[0.70898582]\n",
            " [0.69774739]\n",
            " [0.70755963]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8ZstzAW7AHZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}